
### 开发问题汇集
- PDO 为什么可以防注入
> 使用 PDO 的 prepare 方式，主要是提高相同 SQL 模板查询性能、阻止 SQL 注入；同时，在 PHP 5.3.6 及以前版本中，并不支持在 DSN 中的 charset 定义，而应该使用 PDO::MYSQL_ATTR_INIT_COMMAND 设置初始 SQL, 即我们常用的 set names gbk 指令。  
> 一些程序，还在尝试使用 addslashes 达到防注入的目的，殊不知这样其实问题更多（比如只要注入一些类似0xbf27，然后 addslashes 将它修改为 0xbf5c27，一个合法的多字节字符后面接着一个单引号。换句话说，我可以无视你的转义，成功地注入一个单引号。这是因为 0xbf5c 被当作单字节字符，而非双字节。）；还有一些做法，在执行数据库查询前，将 SQL 中的 select, union, ... 之类的关键词清理掉（比如提交的正文中确实包含 the students's union , 替换后将篡改本来的内容，滥杀无辜，不可取）。  
> 
> PHP 只是简单地将 SQL 直接发送给 MySQL Server，而 PDO 有一项参数，名为 PDO::ATTR_EMULATE_PREPARES，表示是否使用 PHP 本地模拟 prepare，此项参数默认值未知。  
> 使用 PDO 的模拟 prepare，PHP 会将 SQL 模板和变量是分两次发送给 MySQL，由 MySQL 完成变量的转义处理，既然变量和 SQL 模板是分两次发送的，那么就不存在 SQL 注入的问题了，但需要在 DSN 中指定 charset 属性。

- MySQL 的复制原理以及流程
> mysql 要做到主从复制，其实依靠的是二进制日志。  
> 假设主服务器叫 A，从服务器叫 B；主从复制就是 B 跟着 A 学，A 做什么，B 就做什么。那么 B 怎么同步 A 的动作呢？现在 A 有一个日志功能，把自己所做的增删改查的动作全都记录在日志中，B 只需要拿到这份日志，照着日志上面的动作施加到自己身上就可以了。这样就实现了主从复制。  
> 
> 主：binlog 线程 —— 记录下所有改变了数据库数据的语句，放进 master 上的 binlog 中；  
> 从：io 线程 —— 在使用 start slave 之后，负责从 master 上拉取 binlog 内容，放进 自己的 relay log 中；  
> 从：sql 执行线程 —— 执行 relay log 中的语句  

- MySQL 中存储引擎 MyISAM 与 InnoDB 的区别
> 最大的区别是 InnoDB 支持事务，而 MyISAM 不支持事务  
> InnoDB 支持行级锁，而 MyISAM 支持表级锁  
> InnoDB 支持 MVCC, 而 MyISAM 不支持  
> InnoDB 支持外键，而 MyISAM 不支持  
> InnoDB 不支持 FULLTEXT 类型全文索引，而 MyISAM 支持  
> InnoDB 主键查询性能高于 MyISAM，整体上 MyISAM 性能比 InnoDB 高  

- MySQL 中索引方法 btree 和 hash 的区别
> 1.在精确查找的情况下：hash 索引要高于 btree 索引，因为 hash 索引查找数据基本上能一次定位数据（大量 hash 值相等的情况下性能会有所降低，也可能低于 btree）,而 btree 索引基于节点上查找，所以在精确查找方面 hash 索引一般会高于 btree 索引。  
> 2.在范围性查找情况下：比如 'like'等范围性查找 hash 索引无效，因为 hash 算法是基于等值计算的。  
> 3.btree 支持的联合索引的最优前缀；hash 是无法支持的，hash 联合索引要么全用，要么全不用。  
> 4.hash 是不支持索引排序的，索引值和 hash 计算出来的 hash 值大小并不一定一致  
> 
> 因为 hash 结构每个键只对应一个值, 而且是散列的方式分布. 所以他并不支持范围查找和排序等功能；B + 树在查找单条记录的速度虽然比不上 hash 索引, 但是因为更适合排序等操作

- MySQL 中 varchar 与 char 的区别以及 varchar(50) 中的 50 代表的含义
> varchar 与 char 的区别在于 char 是一种固定长度的类型，varchar 则是一种可变长度的类型  
> varchar(50) 中 50 的含义是最多存放 50 个字符，varchar(50) 和 varchar(200) 存储 hello 所占空间一样，但后者在排序时会消耗更多内存，因为 order by column 采用 fixed_length 计算 column 长度(memory 存储引擎也一样)  
> 
> 扩展 int(20) 中 20 的含义  
> int(20）中 20 的含义是指显示字符的长度（默认不补全 0），最大为 255，比如它是记录行数的 id,插入 10 笔资料，它就显示 00000000001 ~~~ 00000000010
> 当字符的位数超过 11,它也只显示 11 位，如果你没有加那个让它未满 11 位就前面加 0 的参数，20 不会显示为 020，但仍占 4 字节存储，存储范围不变；  
> mysql 这么设计对大多数应用是没有意义的，只是规定一些工具用来显示字符的个数；int(1) 和 int(20) 存储和计算均一样  

- 推荐使用 datetime 还是 timestamp 字段？为什么？  
> 正如Mysql文档描述的那样——datetime的范围是“1000-01-01 00:00:00”到“9999-12-31 23:59:59”，而timestamp的范围是 '1970-01-01 00:00:01' UTC 到 '2038-01-09 03:14:07' UTC。如果不想程序在2028年出问题，从这个方面作者建议还是使用datetime。  
> 有一种观点是既不使用 DATETIME 也不使用 TIMESTAMP 字段。如果想将特定的一天作为一个整体来表示（例如生日），可以使用 DATE 类型，但是如果需要更具体的时间，不要使用 DATETIME 或 TIMESTAMP，而是使用 BIGINT，并简单地存储自纪元以来的毫秒数（如果使用的是 Java，则为 System.currentTimeMillis()）。  
> 这样有几个优点。其一，可以在迁移数据库时避免因为数据类型差异，比如MySQL的DATETIME类型和Oracle的DATETIME类型之间可能存在差异，timestamp类型的精度可能也存在差异，MySQL的timestamp精度不是一开始就支持毫秒精度的。其二，没有时区问题，无论是哪个时区，因为开始计算的时间不同，无论当前时间如何，跨度是一致的。其三，没有timestamp和datatime的范围问题，哪怕是datatime，8000年以后也不能用了，没准我的数据库8000年能用8000年呢。  
> 需要注意的是，bigint是占用8个字节，而timestamp只占用4个字节。从MySQL 5.6.4开始，Datetime的存储空间变成了5个字节了(准确的说应该是5字节+0\~3个字节的FSP分秒精度）。


- MySQL 统计 sum 返回 null，那么如何变成 0？
> 使用 COALESCE 即可：SELECT COALESCE(SUM(total),0)  FROM table_name 

- MySQL 统计最近 7 天数据
```mysql
select a.click_date,ifnull(b.count,0) as count_reg  
from (  
    SELECT curdate() as click_date  
    union all  
    SELECT date_sub(curdate(), interval 1 day) as click_date  
    union all  
    SELECT date_sub(curdate(), interval 2 day) as click_date  
    union all  
    SELECT date_sub(curdate(), interval 3 day) as click_date  
    union all  
    SELECT date_sub(curdate(), interval 4 day) as click_date  
    union all  
    SELECT date_sub(curdate(), interval 5 day) as click_date  
    union all  
    SELECT date_sub(curdate(), interval 6 day) as click_date  
) a left join (  
  select date(FROM_UNIXTIME(reg_time)) as datetime, count(*) as count  
  from member  
  group by date(FROM_UNIXTIME(reg_time))  
) b on a.click_date = b.datetime;
```

- 如何从 MySQL 全库备份中恢复某个库和某张表
> 这里主要用到的参数是 --one-database（简写 -o），极大方便了我们的恢复灵活性。
```bash
# 全库备份
mysqldump -uroot -p --single-transaction -A --master-data=2 >dump.sql

# 只还原erp库的内容
mysql -uroot -pMANAGER erp --one-database <dump.sql

# 从全库备份中抽取出 test 表的表结构
sed -e'/./{H;$!d;}' -e 'x;/CREATE TABLE `test`/!d;q' dump.sql

# 从全库备份中抽取出 test 表的内容
grep'INSERT INTO `t`' dump.sql
```

- MySQL 如何支持 emoji 表情
> 使用 utf8_mb4 字符集

- 500 台 db，在最快时间之内重启
> puppet，dsh

- 监控数据库的工具
> 如 zabbix，lepus

- 主从一致性校验的工具
> 如 checksum、mysqldiff、pt-table-checksum

- 如何维护数据库的数据字典
> 我们一般是直接在生产库进行注释，利用工具导出成 excel 方便沟通

- Explain 列的解释
> EXPLAIN 显示了 MySQL 如何使用索引来处理 SELECT 语句以及连接表，可以帮助选择更好的索引和写出更优化的查询语句。  
```
mysql> EXPLAIN SELECT 1;
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra          |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+
|  1 | SIMPLE      | NULL  | NULL       | NULL | NULL          | NULL | NULL    | NULL | NULL |     NULL | No tables used |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+
1 row in set, 1 warning (0.01 sec)

EXPLAIN 列的解释：  
| 列 | 描述 |  
| :-----:  | :----:  |  
| id | 在一个大的查询语句中每个 `SELECT` 关键字都对应一个唯一的 `id` |  
| select_type | `SELECT`关键字对应的那个查询的类型 |  
| table	| 显示这一行的数据是关于哪张表的。 |  
| partitions | 匹配的分区信息 |  
| type | 这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为 const、eq_ref、ref、range、index和ALL。|  
| possible_keys | 显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句。|  
| key |	实际使用的索引。如果为NULL，则没有使用索引。很少的情况下，MySQL 会选择优化不足的索引。这种情况下，可以在SELECT语句中使用USE INDEX（indexname） 来强制使用一个索引或者用IGNORE INDEX（indexname）来强制 MySQL 忽略索引。 |  
| key_len |	使用的索引的长度。在不损失精确性的情况下，长度越短越好。|  
| ref |	显示索引的哪一列被使用了，如果可能的话，是一个常数。|  
| rows | MySQL 认为必须检查的用来返回请求数据的行数。|  
| extra | 关于 MySQL 如何解析查询的额外信息。将在表 4.3 中讨论，但这里可以看到的坏的例子是Using temporary和Using filesort，意思 MySQL 根本不能使用索引，结果是检索会很慢。|  

> type 字段值的意义：  
> all — 扫描全表数据  
> index — 遍历索引  
> range — 索引范围查找  
> index_subquery — 在子查询中使用 ref  
> uniquesubquery — 在子查询中使用 eqref   
> refornull — 对 null 进行索引的优化的 ref  
> fulltext — 使用全文索引  
> ref — 使用非唯一索引查找数据，在联合索引中常见  
> eq_ref — 在 join 查询中使用主键或唯一索引关联  
> const — 将一个主键放置到 where 后面作为条件查询， MySQL 优化器就能把这次查询优化转化为一个常量，如何转化以及何时转化，这个取决于优化器，这个比 eq_ref 效率高一点  

> Extra 列返回的描述的意义：  
| 值 | 意义 |  
| :-----:  | :----:  |
| Distinct | 一旦 MySQL 找到了与行相联合匹配的行，就不再搜索了。|  
| Not exists | MySQL 优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了。|  
| Range checked for each Record（index map:#）| 没有找到理想的索引，因此对于从前面表中来的每一个行组合，MySQL 检查使用哪个索引，并用它来从表中返回行。这是使用索引的最慢的连接之一。|  
| Using filesort | 看到这个的时候，查询就需要优化了。MySQL 需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来排序全部行。|  
| Using index | 列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候。|  
| Using temporary | 看到这个的时候，查询需要优化了。这里，MySQL 需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上。|  
| Where used | 	使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，这就会发生，或者是查询有问题不同连接类型的解释（按照效率高低的顺序排序）。|  
| system | 表只有一行 system 表。这是 const 连接类型的特殊情况 。|  
| const | 表中的一个记录的最大值能够匹配这个查询（索引可以是主键或惟一索引）。因为只有一行，这个值实际就是常数，因为 MySQL 先读这个值然后把它当做常数来对待。|  
| eq_ref | 在连接中，MySQL 在查询时，从前面的表中，对每一个记录的联合都从表中读取一个记录，它在查询使用了索引为主键或惟一键的全部时使用。|  
| ref	| 这个连接类型只有在查询使用了不是惟一或主键的键或者是这些类型的部分（比如，利用最左边前缀）时发生。对于之前的表的每一个行联合，全部记录都将从表中读出。这个类型严重依赖于根据索引匹配的记录多少—越少越好。|  
| range	| 这个连接类型使用索引返回一个范围中的行，比如使用 > 或 < 查找东西时发生的情况。|  
| index	| 这个连接类型对前面的表中的每一个记录联合进行完全扫描（比ALL更好，因为索引一般小于表数据）。|  
| ALL	| 这个连接类型对于前面的每一个记录联合进行完全扫描，这一般比较糟糕，应该尽量避免。|  

EXPLAIN SELECT `surname`,`first_name` FORM `a`,`b` WHERE `a`.`id`=`b`.`id`
```

- 数据库设计和查询优化
> Schema 设计时主要考虑: 标准化,数据类型,索引  
> 一个数据库设计可以混合使用,一部分表格标准化,一部分表格非标准化(非标准化表格适当冗余)  
> 最优的数据类型,使表在磁盘上占据的空间尽可能小,读写快,占用内存少(索引也尽量建立在较小的列上)  
> 正确索引,提高 Select,Update,Delete 性能  
> 
> 不同的Sql不同的优化方案  
> Explain Sql 查看结果,分析查询  
> 查询使用匹配的类型  
> 使用 long-slow-queries 记录较慢查询,分析优化  
>
> 服务器端优化  
> 安装适当的 MySql 版本  
> 如果服务器使用 Intel 处理器, 使用 Intel C++ 版本可提高 30 % 效率  
> 
> 配置优化，常见优化项:  
> charset  
> max_allowed_packet  
> max_connections  
> table_cache_size  
> query_cache_size  
> 
> 存储引擎优化  
> MyISAM  
> MyISAM 引擎特点  
> 不支持事务, 提供高速存储, 检索以及全文搜索能力.  
> 宕机会破坏表.  
> 使用的磁盘和内存空间小.  
> 基于表的锁, 并发更新数据会出现严重性能问题.  
> MySql 只缓存索引, 数据由 OS 缓存.  
> MyISAM 适用情况
> 日志系统.  
> 只读操作或者大部分读操作.  
> 全表扫描.  
> 批量导入数据.  
> 没有事务的低并发读写.  
> MyISAM 优化策略  
> NOT NULL, 可以减少磁盘存储.  
> Optimize Table, 碎片整理, 回收空闲空间.  
> Deleting/updating/adding 大量数据的时候禁止使用 index.  
> 参数优化, key_buffer_size_variable 索引缓存设置.  
> 避免并发 Inset Update.  
>
> InnoDB  
> InnoDB 引擎特点  
> 具有提交, 回滚和崩溃恢复能力的事务安全存储引擎.  
> 处理巨大数据量性能卓越, 它的 CPU 使用效率非常高.  
> 需要更多的内存和磁盘存储空间.  
> 数据和索引都缓存在内存中.  
> InnoDB 适用情况  
> 需要事务的应用.  
> 高并发的应用.  
> 自动恢复.  
> 较快速的基于主键的操作.  
> InnoDB 优化策略  
> 尽量使用 short,integer 的主键.  
> 使用 prefix keys, 因为 InnoDB 没有 key 压缩功能.  
> 参数优化, innodb_buffer_pool_size,innodb_data_home_dir 等等  

- mysql sql 调优
> MySQL 里最直接的优化就是保证减少 io 请求，尽量 90% 多业务走单表扫描，需要计算或者关联的业务，尽量放在程序层完成。然而此时就会有疑问，都单表了，还有什么好优化的呢？  
> 如果一个线上业务 SQL 比较慢，十有八九就是因为那个 SQL 没有用索引，所以这个时候，第一步就是去看 MySQL 的执行计划，看看那个 SQL 有没有用到索引，如果没有，那么就改写一下 SQL 让他用上索引，或者是额外加个索引。   
> 关于 mysql 执行计划，可以用 explain 或 explain extended 分析下执行计划，重点关注下 key（用到那个索引）、rows（扫描行数）、extra：using filesort（需要额外进行排序），using temporary（mysql 构建了临时表，比如排序的时候），using where（就是对索引扫出来的数据再次根据 where 来过滤出了结果）重点的信息，基本也就能定位 sql 性能的问题了。  
> 
> 性能调优包括如下几个方面：  
> 1、sql 语句的执行计划是否正常  
> 2、减少应用和数据库的交互次数、同一个 sql 语句的执行次数  
> 3、数据库实体的碎片的整理（特别是对某些表经常进行 insert 和 delete 动作，尤其注意，索引字段为系列字段、自增长字段、时间字段，对于业务比较频繁的系统，最好一个月重建一次）  
> 4、减少表之间的关联，特别对于批量数据处理，尽量单表查询数据，统一在内存中进行逻辑处理，减少数据库压力（java 处理批量数据不可取，尽量用 c 或者 c++ 进行处理，效率大大提升）  
> 5、对访问频繁的数据，充分利用数据库 cache 和应用的缓存  
> 6、数据量比较大的，在设计过程中，为了减少其他表的关联，增加一些冗余字段，提高查询性能

- mysql 中 in 和 exists 区别
> mysql 中的 in 语句是把外表和内表作 hash 连接，而 exists 语句是对外表作 loop 循环，每次 loop 循环再对内表进行查询。一直大家都认为 exists 比 in 语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。  
> 如果查询的两个表大小相当，那么用 in 和 exists 差别不大。
> 如果两个表中一个较小，一个是大表，则子查询表大的用 exists，子查询表小的用 in。
> not in 和 not exists 如果查询语句使用了 not in 那么内外表都进行全表扫描，没有用到索引；而 not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用 not exists 都比 not in 要快。

- 索引设计有哪些原则
> 适合索引的列是出现在 where 子句中的列，或者连接子句中指定的列  
> 基数较小的类，索引效果较差，没有必要在此列建立索引  
> 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间  
> 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。

- mysql 主从同步如何实现
> 数据库有个 bin-log 二进制文件，记录了所有的 sql 语句；只需要把主数据库的 bin-log 文件中的 sql 语句复制，让其在从数据库的 relay-log 重做日志文件中在执行一次这些 sql 语句即可。  
> 主从的意义：做数据的热备份，作为后备数据库，主数据库服务器故障后，可切换到从数据库继续工作，避免数据丢失；架构的扩展。业务量越来越大，I/O 访问频率过高，单机无法满足，此时做多库的存储，降低磁盘 I/O 访问频率，提高单机的 I/O 性能；主从复制是读写分离的基础，使数据库能制成更大的并发。例如子报表中，由于部署报表的 sql 语句十分慢，导致锁表，影响前台的服务。如果前台服务使用 master，报表使用 slave，那么报表 sql 将不会造成前台所，保证了前台的访问速度。  

- 主从复制的几种方式
> 1.同步复制：所谓的同步复制，意思是 master 的变化，必须等待 slave-1,slave-2,...,slave-n 完成后才能返回。  
> 2.异步复制：如同 AJAX 请求一样。master 只需要完成自己的数据库操作即可。至于 slaves 是否收到二进制日志，是否完成操作，不用关心。MYSQL 的默认设置。  
> 3.半同步复制：master 只保证 slaves 中的一个操作成功，就返回，其他 slave 不管。
这个功能，是由 google 为 MYSQL 引入的。  


- 悲观锁和乐观锁
> 悲观锁——总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。  
> 乐观锁——总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和 CAS 算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。  
> 
> 乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行 retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。  
> 
> 乐观锁一般会使用版本号机制或 CAS 算法实现。  
> 版本号机制——一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数，当数据被修改时，version 值会加一。当线程 A 要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值为当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功。  
> CAS 算法——即 compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS 算法涉及到三个操作数：需要读写的内存值 V、进行比较的值 A、拟写入的新值 B，当且仅当 V 的值等于 A 时，CAS 通过原子方式用新值 B 来更新 V 的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。  
> 
> 乐观锁的缺点  
> 如果一个变量 V 初次读取的时候是 A 值，并且在准备赋值的时候检查到它仍然是 A 值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回 A，那 CAS 操作就会误认为它从来没有被修改过。这个问题被称为 CAS 操作的 "ABA" 问题。  
> 自旋 CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给 CPU 带来非常大的执行开销。  
> CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。  

- MySQL 分表分库全局 ID
> mysql 分库分表确实能解决不少问题，也能让数据库支撑更大的并发、大数据量业务，可是分库分表后必然面对的一个问题就是 id 怎么生成？id 生成后如何保持全局唯一性。  
> 1、数据库自增 id  
> 系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。这样确实可以解决，但是在高并发业务场景下，就会发生瓶颈，瞬间那一刻会出现脏数据。所以这个办法只能适合非高并发下的分表分库，如何生成全局 id。  
> 2、uuid  
> 本地生成，不要基于数据库来了；不好之处就是，uuid 太长了，作为主键性能太差了，不适合用于主键。如果你是要随机生成个什么文件名了，编号之类的，你可以用 uuid，但是作为主键是不能用 uuid 的。  
> 3、获取当前系统时间  
> 这个就是获取当前时间即可，但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个是肯定不合适的。一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的标识，订单号，时间戳 + 用户 id + 业务唯一标识。  
> 4、snowflake 算法  
> 在系统小时，唯一标识的产生，可以利用公用模块来处理，比如数据库表的唯一键、或者缓存的唯一 id 等等方式。但在分布式高并发的系统中，如果还是这样使用公共模块，就会产生很大的风险和瓶颈。  
> snowflake 是 Twitter 开源的分布式 ID 生成算法，结果是一个 long 型的 ID。其核心思想是：使用 41 bit 作为毫秒数，10 bit 作为机器的 ID（5 个 bit 是数据中心，5 个 bit 的机器 ID），12bit 作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是 0。  
> 这个算法单机每秒内理论上最多可以生成 1000 * (2 ^ 12)，也就是 409.6 万个 ID。  
 
- Mysql 的死锁问题
> 原理：当对于数据库某个表的某一列做更新或删除等操作，执行完毕后该条语句不提交，另一条对于这一列数据做更新操作的语句在执行的时候就会处于等待状态，此时的现象是这条语句一直在执行，但一直没有执行成功，也没有报错。  
> 
> 定位：  
> 1、用 dba 用户执行以下语句
> select username,lockwait,status,machine,program from v$session where sid in(select session_id from v$locked_object)  
> 如果有输出的结果，则说明有死锁，且能看到死锁的机器是哪一台（username 死锁语句所用的数据库用户；lockawait 死锁的状态，如果有内容表示被死锁；status 状态，active 表示被死锁；machine 死锁语句所在的机器；program 产生死锁的语句主要来自哪个应用程序）  
> 2、用 dba 用户执行以下语句，可以查看到被死锁的语句  
> select sql_text from v$sql where hash_value in (select sql_hash_value from v$session where sid in(select session_id from v$locked_object))  
> 
> 解决方案：  
> 一般情况下，只要将产生死锁的语句提交就可以了，但是在实际的执行过程中。用户可能不知道产生死锁的语句是哪一句。可以将程序关闭并重新启动就可以了。  
> 查找死锁的进程：sqlplus "/as sysdba" (sys/change_on_install) SELECT s.username,l.OBJECT_ID,l.SESSION_ID,s.SERIAL#,l.ORACLE_USERNAME,l.OS_USER_NAME,l.PROCESS FROM V$LOCKED_OBJECT l,V$SESSION S WHERE l.SESSION_ID=S.SID;  
> kill 掉这个死锁的进程：alter system kill session ‘sid,serial#’; （其中 sid=l.session_id）  
> 如果还不能解决：select pro.spid from v$session ses,v$process pro where ses.sid=XX and ses.paddr=pro.addr;（其中，sid 用死锁的 sid 替换：ps -ef|grep spid）  

- 如何构建高效的 mysql 分页功能
> 1、将 LIMIT M,N 的查询改为 LIMIT N  
> 例如，使用 LIMIT 10000,20，Mysql 将需要读取前 10000 行，然后获取后面的 20 行 ，这是非常低效的，使用 LIMIT N 的方式，通过每页第一条或最后一条记录的 id 来做条件筛选，再配合降序和升序获得上 / 下一页的结果集 。  
> 2、限制用户翻页数量  
> 产品实际使用过程中用户很少关心搜索结果的第 1 万条数据。  
> 3、使用延迟关联  
> 通过使用覆盖索引来查询返回需要的主键，再根据返回的主键关联原表获得需要的行，这样可以减少 Mysql 扫描那些需要丢弃的行数。

- 值为空和值为 null 的区别
> 空值不一定为空。空值是一个比较特殊的字段。  
> 在 MySQL 数据库中，在不同的情形下，空值往往代表不同的含义。这是 MySQL 数据库的一种特性。如在普通的字段中 (字符型的数据)，空值就是表示空值。但是如果将一个空值的数据插入到 TimesTamp 类型的字段中，空值就不一定为空。
> 空值 ('') 是不占用空间的，MySQL 中的 NULL 其实是占用空间的。

- Mysql 中的覆盖索引
> 解释一： 就是 select 的数据列只用从索引中就能够取得，不必从数据表中读取，换句话说查询列要被所使用的索引覆盖。  
> 解释二： 索引是高效找到行的一个方法，当能通过检索索引就可以读取想要的数据，那就不需要再到数据表中读取行了。如果一个索引包含了（或覆盖了）满足查询语句中字段与条件的数据就叫做覆盖索引。  
> 解释三：是非聚集组合索引的一种形式，它包括在查询里的 Select、Join 和 Where 子句用到的所有列（即建立索引的字段正好是覆盖查询语句 [select 子句] 与查询条件 [Where 子句] 中所涉及的字段，也即，索引包含了查询正在查找的所有数据）。  

- sql 语句的执行顺序
> 1、from 子句组装来自不同数据源的数据；  
> 2、where 子句基于指定的条件对记录行进行筛选；  
> 3、group by 子句将数据划分为多个分组；  
> 4、使用聚集函数进行计算；  
> 5、使用 having 子句筛选分组；  
> 6、计算所有的表达式；  
> 7、使用 order by 对结果集进行排序；  
> 8、select 集合输出  
> 1）语法分析，分析语句的语法是否符合规范，衡量语句中各表达式的意义。  
> 2）语义分析，检查语句中涉及的所有数据库对象是否存在，且用户有相应的权限。  
> 3）视图转换，将涉及视图的查询语句转换为相应的对基表查询语句。  
> 4）表达式转换， 将复杂的 SQL 表达式转换为较简单的等效连接表达式。  
> 5）选择优化器，不同的优化器一般产生不同的 “执行计划”  
> 6）选择连接方式， ORACLE 有三种连接方式，对多表连接 ORACLE 可选择适当的连接方式。  
> 7）选择连接顺序， 对多表连接 ORACLE 选择哪一对表先连接，选择这两表中哪个表做为源数据表。  
> 8）选择数据的搜索路径，根据以上条件选择合适的数据搜索路径，如是选用全表搜索还是利用索引或是其他的方式。  
> 9）运行 “执行计划”

- left join、right join、full join 和 inner join 的区别
> left join (左连接) 返回包括左表中的所有记录和右表中连接字段相等的记录；  
> right join (右连接) 返回包括右表中的所有记录和左表中连接字段相等的记录；  
> full join（全连接）查询结果是左连接和右连接的并集。  
> 左连接和右连接很相似，只是左右表位置的不同罢了。适用场景，如：员工表中有个字段是详细地址信息表的主键 id，这两个表相关联时，就可以用左连接或右连接，因为在详细地址信息表中找不到某员工的地址信息也要将员工这条记录显示出来，相应的详细地址信息字段为空即可，而不能因为地址没有存在数据库里，这个员工就没了（简单理解成不重要的信息不影响整条记录的显示）。  
> inner join (等值连接、内连接) 只返回两个表中连接字段相等的行。  
> 内连接的适用场景：相连接的两个表中必须在某个字段上有相等的值才可以将整条记录显示出来，如一条服务单记录在了两个表中，A 表中记录了该服务单的服务时间、坐席名称和录音地址等基本信息，B 表中记录了该服务单的业务详情，如保险单号，车牌号，保单日期等，当显示该服务单时，要将 A 表与 B 表做内连接，因为少这两表任何一个表，该服务单都不算完整，缺失的信息会使业务上没法继续。  
> 总结：可以将可使用左连接和右连接的两个表理解成其中一个表的信息明显比另外一个表的信息重要得多；使用内连接的两个表理解成重要程度区别不太大的两个表。  
> 
> 内连接，外链接的区别？  
> 内连接，也被称为自然连接，只有两个表相匹配的行才能在结果集中出现。返回的结果集选取了两个表中所有相匹配的数据，舍弃了不匹配的数据。由于内连接是从结果表中删除与其他连接表中没有匹配的所有行，所以内连接可能会造成信息的丢失。  
> 内连接语法如下：select fieldlist from table1 [inner] join table2 on table1.column = table2.column  
> 内连接是保证两个表中所有行都满足连接条件，而外连接则不然。  
> 外连接不仅包含符合连接条件的行，还包含左表 (左连接时)、右表 (右连接时) 或两个边接表 (全外连接) 中的所有数据行。SQL 外连接共有三种类型：左外连接 (关键字为 LEFT OUTER JOIN)、右外连接 (关键字为 RIGHT OUTER JOIN) 和全外连接 (关键字为 FULL OUTER JOIN)。外连接的用法和内连接一样，只是将 INNER JOIN 关键字替换为相应的外连接关键字即可。  
> 内连接：指连接结果仅包含符合连接条件的行，参与连接的两个表都应该符合连接条件。  
> 外连接：连接结果不仅包含符合连接条件的行同时也包含自身不符合条件的行。包括左外连接、右外连接和全外连接。  
> 左外连接：左边表数据行全部保留，右边表保留符合连接条件的行。  
> 右外连接：右边表数据行全部保留，左边表保留符合连接条件的行。  
> 全外连接：左外连接 union 右外连接。
 
- mysql 连接池
> 什么是数据库连接池？  
> 数据库连接池（Connection pooling）是程序启动时建立足够的数据库连接，并将这些连接组成一个连接池，由程序动态地对池中的连接进行申请，使用，释放。  
> 创建数据库连接是一个很耗时的操作，也容易对数据库造成安全隐患。所以，在程序初始化的时候，集中创建多个数据库连接，并把他们集中管理，供程序使用，可以保证较快的数据库读写速度，还更加安全可靠。  
> 
> 数据库连接池的运行机制：  
> （1） 程序初始化时创建连接池  
> （2） 使用时向连接池申请可用连接  
> （3） 使用完毕，将连接返还给连接池  
> （4） 程序退出时，断开所有连接，并释放资源  
> 
> 数据库连接是一种关键的有限的昂贵的资源，这一点在多用户的网页应用程序中体现的尤为突出。对数据库连接的管理能显著影响到整个应用程序的伸缩性和健壮性，影响到程序的性能指标。数据库连接池正式针对这个问题提出来的。数据库连接池负责分配，管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。   
> 数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。  
>   
> 使用 mysql 连接池关键点：  
> 1、并发问题  
> 为了使连接管理服务具有最大的通用性，必须考虑多线程环境，即并发问题。这个问题相对比较好解决，因为各个语言自身提供了对并发管理的支持像 java,c# 等等，使用 synchronized (java) lock (C#) 关键字即可确保线程是同步的。  
> 2、事务处理  
> DB 连接池必须要确保某一时间内一个 conn 只能分配给一个线程。不同 conn 的事务是相互独立的。  
> 我们知道，事务具有原子性，此时要求对数据库的操作符合 “ALL-ALL-NOTHING” 原则，即对于一组 SQL 语句要么全做，要么全不做。  
> 我们知道当２个线程共用一个连接 Connection 对象，而且各自都有自己的事务要处理时候，对于连接池是一个很头疼的问题，因为即使 Connection 类提供了相应的事务支持，可是我们仍然不能确定那个数据库操作是对应那个事务的，这是由于我们有２个线程都在进行事务操作而引起的。为此我们可以使用每一个事务独占一个连接来实现，虽然这种方法有点浪费连接池资源但是可以大大降低事务管理的复杂性。  
> ３、连接池的分配与释放  
> 连接池的分配与释放，对系统的性能有很大的影响。合理的分配与释放，可以提高连接的复用度，从而降低建立新连接的开销，同时还可以加快用户的访问速度。  
> 对于连接的管理可使用一个 List。即把已经创建的连接都放入 List 中去统一管理。每当用户请求一个连接时，系统检查这个 List 中有没有可以分配的连接。如果有就把那个最合适的连接分配给他（如何能找到最合适的连接将在关键议题中指出）；如果没有就抛出一个异常给用户，List 中连接是否可以被分配由一个线程来专门管理捎后我会介绍这个线程的具体实现。  
> ４、连接池的配置与维护  
> 连接池中到底应该放置多少连接，才能使系统的性能最佳？系统可采取设置最小连接数（minConnection）和最大连接数（maxConnection）等参数来控制连接池中的连接。比方说，最小连接数是系统启动时连接池所创建的连接数。如果创建过多，则系统启动就慢，但创建后系统的响应速度会很快；如果创建过少，则系统启动的很快，响应起来却慢。这样，可以在开发时，设置较小的最小连接数，开发起来会快，而在系统实际使用时设置较大的，因为这样对访问客户来说速度会快些。最大连接数是连接池中允许连接的最大数目，具体设置多少，要看系统的访问量，可通过软件需求上得到。  
> 如何确保连接池中的最小连接数呢？有动态和静态两种策略。动态即每隔一定时间就对连接池进行检测，如果发现连接数量小于最小连接数，则补充相应数量的新连接，以保证连接池的正常运转。静态是发现空闲连接不够时再去检查。  
> 
> 连接池的主要优点有以下三个方面：  
> 第一、减少连接创建时间。连接池中的连接是已准备好的、可重复使用的，获取后可以直接访问数据库，因此减少了连接创建的次数和时间。  
> 第二、简化的编程模式。当使用连接池时，每一个单独的线程能够像创建一个自己的 JDBC 连接一样操作，允许用户直接使用 JDBC 编程技术。   
> 第三、控制资源的使用。如果不使用连接池，每次访问数据库都需要创建一个连接，这样系统的稳定性受系统连接需求影响很大，很容易产生资源浪费和高负载异常。连接池能够使性能最大化，将资源利用控制在一定的水平之下。连接池能控制池中的连接数量，增强了系统在大量用户应用时的稳定性。  
> 
> 连接池的工作原理：  
> 连接池技术的核心思想是连接复用，通过建立一个数据库连接池以及一套连接使用、分配和管理策略，使得该连接池中的连接可以得到高效、安全的复用，避免了数据库连接频繁建立、关闭的开销。  
> 连接池的工作原理主要由三部分组成，分别为连接池的建立、连接池中连接的使用管理、连接池的关闭。  
> 第一、连接池的建立。一般在系统初始化时，连接池会根据系统配置建立，并在池中创建了几个连接对象，以便使用时能从连接池中获取。连接池中的连接不能随意创建和关闭，这样避免了连接随意建立和关闭造成的系统开销。Java 中提供了很多容器类可以方便的构建连接池，例如 Vector、Stack 等。  
> 第二、连接池的管理。连接池管理策略是连接池机制的核心，连接池内连接的分配和释放对系统的性能有很大的影响。其管理策略是：当客户请求数据库连接时，首先查看连接池中是否有空闲连接，如果存在空闲连接，则将连接分配给客户使用；如果没有空闲连接，则查看当前所开的连接数是否已经达到最大连接数，如果没达到就重新创建一个连接给请求的客户；如果达到就按设定的最大等待时间进行等待，如果超出最大等待时间，则抛出异常给客户。    
> 当客户释放数据库连接时，先判断该连接的引用次数是否超过了规定值，如果超过就从连接池中删除该连接，否则保留为其他客户服务。  
> 该策略保证了数据库连接的有效复用，避免频繁的建立、释放连接所带来的系统资源开销。  
> 第三、连接池的关闭。当应用程序退出时，关闭连接池中所有的连接，释放连接池相关的资源，该过程正好与创建相反。  
> 
> 常用的连接池：  
> dbcp 可能是使用最多的开源连接池，原因大概是因为配置方便，而且很多开源和 tomcat 应用例子都是使用的这个连接池吧。这个连接池可以设置最大和最小连接，连接等待时间等，基本功能都有。这个连接池的配置在具体项目应用中，发现此连接池的持续运行的稳定性还是可以，不过速度稍慢，在大并发量的压力下稳定性。  

- 分析 MySQL 查询慢的原因
> 1.查看慢查询日志  
> 2.通过 pt-query-digest 工具分析  
> 3.设置 set profiling = 1;开启服务，执行 show profile。查看所有语句会监测消耗时间存到临时表。  
> 4.找到消耗时间大的 ID，执行 show profile for query 临时表 ID  
> 5.使用 show status，show processlist 等命令查看
> 6.使用 explain 分析单条 SQL 语句

- count(column) 和 count(\*) 有什么区别？  
> count(column) 和 count(\*) 最大区别是统计结果可能不一致，count(column) 统计不会统计列值为 null 的数据，而 count(\*) 则会统计所有信息，所以最终的统计结果可能会不同。  
> count 在 InnoDB 中是一行一行读取，然后累计计数的。  

- 为什么 InnoDB 不把总条数记录下来，查询的时候直接返回呢？  
> 因为 InnoDB 使用了事务实现，而事务的设计使用了多版本并发控制，即使是在同一时间进行查询，得到的结果也可能不相同，所以 InnoDB 不能把结果直接保存下来，因为这样是不准确的。

- 在 InnoDB 引擎，count 性能消耗为什么是这样 count (字段) < count(主键 id) < count (1)≈count(\*) ？ 
> 对于 count (主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。  
> 对于 count (1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字 “1” 进去，判断是不可能为空的，按行累加。  
> 对于 count (字段) 来说，如果这个 “字段” 是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；如果这个 “字段” 定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。  
> 对于 count (\*) 来说，并不会把全部字段取出来，而是专门做了优化，不取值，直接按行累加。  

- 使用 having 有什么需要注意的？  
> select 字段中必须包含 having 使用的字段。  

- 空值和 null 的区别？  
> 空值表示字段的值为空，而 NULL 则表示字段没有值。  
> 空值不占用空间，NULL 值是未知的占用空间；  
> 空值判断使用 ='' 或 <>'' 来判断，NULL 值使用 IS NULL 或 IS NOT NULL 来判断；  
> 使用 COUNT 统计某字段时，如果是 NULL 则会忽略不统计，而空值则会算入统计之内。  

- 有一个超级大表，如何优化分页查询？
> 数据库层面优化：利用子查询优化超多分页场景，比如：SELECT a.* FROM 表 1 a, (select id from 表 1 where 条件 LIMIT 100000,20 ) b where a.id=b.id ，先快速定位需要获取的 id 段，然后再关联查询。MySQL 并不是跳过 offset 行，而是取 offset+N 行，然后返回放弃前 offset 行，返回 N 行，那当 offset 特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行 SQL 改写，利用子查询先快速定位需要获取的 id 段，然后再关联查询，就是对分页进行 SQL 改写的具体实现；  
> 程序层面优化：可以利用缓存把查询的结果缓存起来，下一次查询的时候性能就会非常高。  
```sql
# 分段扫描
select id from table_name where id>73575000 order by id asc limit 0, 5000;
select a2.id,a2.keyword,a2.url from (select id from table_name where id>73575000 order by id asc limit 0, 5000) a1, table_name a2 where a1.id=a2.id
# 或者直接这样
select id,keyword,url  from table_name where id>73575000 order by id asc limit 0, 5000
```

- 请写出 mysql 分页查找返回，并返回匹配总统计数？  
```sql
select sql_calc_found_rows column_name from table_name limit offset, size;
select found_rows(); # 返回查询记录的总数
```

- 线上修改表结构有哪些风险？  
> 在 MySQL 5.6 开始提供了 online ddl 功能，允许一些 DDL（create table/view/index/syn/cluster）语句和 DML 语句并发，在 5.7 版本对 online ddl 又有了增强，这使得大部分 DDL 操作可以在线进行，[详见这里](https://dev.mysql.com/doc/refman/5.7/en/innodb-create-index-overview.html)，这使得在线上修改表结构的风险变的更大，如果在业务开发过程中必须在线修改表结构，可以参考以下方案：  
> 尽量在业务量小的时间段进行；  
> 查看官方文档，确认要做的表修改可以和 DML 并发，不会阻塞线上业务；  
> 推荐使用 percona 公司的 pt-online-schema-change 工具，该工具被官方的 online ddl 更为强大，它的基本原理是：通过 insert...select... 语句进行一次全量拷贝，通过触发器记录表结构变更过程中产生的增量，从而达到表结构变更的目的。  

- 查询长时间不返回可能是什么原因？应该如何处理？
> 可能的原因如下：  
> 1）查询字段没有索引或者没有触发索引查询  
> 2）I/O 压力大，读取磁盘速度变慢  
> 3）内存不足  
> 4）网络速度慢  
> 5）查询出的数据量过大，可以采用多次查询或其他的方法降低数据量  
> 6）死锁，设置死锁的超时时间，限制和避免死锁消耗过多服务器的资源  

- MySQL 主从延迟的原因有哪些？  
> 主从复制延迟会带来一个问题：如果业务服务器将数据写入到数据库主服务器后立刻（1 秒内）进行读取，此时读操作访问的是从机，主机还没有将数据复制过来，到从机读取数据是读不到最新数据的，业务上就可能出现问题。例如，用户刚注册完后立刻登录，业务服务器会提示他“你还没有注册”，而用户明明刚才已经注册成功了。  
> 主从延迟可以根据 MySQL 提供的命令判断，比如，在从服务器使用命令： show slave status;，其中 SecondsBehindMaster 如果为 0 表示主从复制状态正常。 导致主从延迟的原因有以下几个：  
> 主库有大事务处理；  
> 主库做大量的增、删、改操作；  
> 主库对大表进行字段新增、修改或添加索引等操作；  
> 主库的从库太多，导致复制延迟。从库数量一般 3-5 个为宜，要复制的节点过多，导致复制延迟；  
> 从库硬件配置比主库差，导致延迟。查看 Master 和 Slave 的配置，可能因为从库的配置过低，执行时间长，由此导致的复制延迟时间长；  
> 主库读写压力大，导致复制延迟；  
> 从库之间的网络延迟。主从库网卡、网线、连接的交换机等网络设备都可能成为复制的瓶颈，导致复制延迟，另外跨公网主从复制很容易导致主从复制延迟。 
  
> 
> 解决主从复制延迟有几种常见的方法：  
> 1. 写操作后的读操作指定发给数据库主服务器  
> 例如，注册账号完成后，登录时读取账号的读操作也发给数据库主服务器。这种方式和业务强绑定，对业务的侵入和影响较大，如果哪个新来的程序员不知道这样写代码，就会导致一个 bug。  
> 2. 读从机失败后再读一次主机  
> 这就是通常所说的“二次读取”，二次读取和业务无绑定，只需要对底层数据库访问的 API 进行封装即可，实现代价较小，不足之处在于如果有很多二次读取，将大大增加主机的读操作压力。例如，黑客暴力破解账号，会导致大量的二次读取操作，主机可能顶不住读操作的压力从而崩溃。  
> 3. 关键业务读写操作全部指向主机，非关键业务采用读写分离  
> 例如，对于一个用户管理系统来说，注册 + 登录的业务读写操作全部访问主机，用户的介绍、爱好、等级等业务，可以采用读写分离，因为即使用户改了自己的自我介绍，在查询时却看到了自我介绍还是旧的，业务影响与不能登录相比就小很多，还可以忍受。  

- 怎样保证确保备库无延迟？  
> 通常保证主备无延迟有以下三种方法：  
> 1、每次从库执行查询请求前，先判断 secondsbehindmaster 是否已经等于 0。如果还不等于 0 ，那就必须等到这个参数变为 0 才能执行查询请求，secondsbehindmaster 参数是用来衡量主备延迟时间的长短；  
> 2、对比位点确保主备无延迟。MasterLogFile 和 ReadMasterLogPos，表示的是读到的主库的最新位点，RelayMasterLogFile 和 ExecMasterLog_Pos，表示的是备库执行的最新位点；  
> 3、对比 GTID 集合确保主备无延迟。AutoPosition=1 ，表示这对主备关系使用了 GTID 协议；RetrievedGtidSet，是备库收到的所有日志的 GTID 集合；ExecutedGtid_Set，是备库所有已经执行完成的 GTID 集合。  

- 如何保证数据不被误删？
> 权限控制与分配（数据库和服务器权限）；  
> 避免数据库账号信息泄露，在生产环境中，业务代码不要使用明文保存数据库连接信息；  
> 重要的数据库操作，通过平台型工具自动实施，减少人工操作；  
> 部署延迟复制从库，万一误删除时用于数据回档，且从库设置为 read-only；  
> 确认备份制度及时有效；  
> 启用 SQL 审计功能，养成良好 SQL 习惯；  
> 启用 sqlsafeupdates 选项，不允许没 where 条件的更新 / 删除；  
> 将系统层的 rm 改为 mv；  
> 线上不进行物理删除，改为逻辑删除（将 row data 标记为不可用）；  
> 启用堡垒机，屏蔽高危 SQL；  
> 降低数据库中普通账号的权限级别；  
> 开启 binlog，方便追溯数据。  

- Mysql 如何确保数据不丢失？  
> 日志先行，io 顺序写，异步操作，做到了高效操作。  
> 对数据页，先在内存中修改，然后使用 io 顺序写的方式持久化到 redo log 文件；然后异步去处理 redo log，将数据页的修改持久化到磁盘中，效率非常高，整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。  
> 
> 两阶段提交确保 redo log 和 binlog 一致性。  
> 为了确保 redo log 和 binlog 一致性，此处使用了二阶段提交技术，redo log 和 binlog 的写分了 3 步走：  
> 1、携带 trx_id，redo log prepare 到磁盘  
> 2、携带 trx_id，binlog 写入磁盘  
> 3、携带 trx_id，redo log commit 到磁盘  
> 上面 3 步骤，可以确保同一个 trx_id 关联的 redo log 和 binlog 的可靠性。  

- MySQL 服务器 CPU 飙升应该如何处理？  
> 使用 show full processlist; 查出慢查询，为了缓解数据库服务器压力，先使用 kill 命令杀掉慢查询的客户端；  
> 然后再去项目中找到执行慢的 SQL 语句进行修改和优化。  

- MySQL 毫无规律的异常重启，可能产生的原因是什么？该如何解决？  
> 可能是积累的长连接导致内存占用太多，被系统强行杀掉导致的异常重启，因为在 MySQL 中长连接在执行过程中使用的临时内存对象，只有在连接断开的时候才会释放，这就会导致内存不断飙升，解决方案如下：  
> 定期断开空闲的长连接；  
> 如果是用的是 MySQL 5.7 以上的版本，可以定期执行 mysqlresetconnection 重新初始化连接资源，这个过程会释放之前使用的内存资源，恢复到连接刚初始化的状态。

- 如何实现一个高并发的系统？  
> 在互联网时代，所讲的并发、高并发，通常是指并发访问。也就是在某个时间点，有多少个访问同时到来通常如果一个系统的日 PV 在千万以上，有可能是一个高并发的系统，但是有的公司完全不走技术路线，全靠机器堆，这不在我们的讨论范围。  
> 通常，我们关注如下指标：QPS（每秒请求响应数）、吞吐量（单位时间内处理的请求数）、PV（综合浏览量）、UV（独立访问量）及带宽（日网站带宽 = PV / 统计时间 (换算到秒)* 平均页面大小 (单位 KB)\*8）。  
> 随着 QPS 的增长，每个阶段需要根据实际情况来进行优化，优化的方案也与硬件条件、网络带宽息息相关。  
> QPS 达到 50 —— 可以称之为小型网站，一般的服务器就可以应付；  
> QPS 达到 100 —— 数据库缓存层、数据库的负载均衡；  
> QPS 达到 800 —— CDN 加速、负载均衡；  
> QPS 达到 1000 —— 静态 HTML 缓存；  
> QPS 达到 2000 —— 做业务分离，分布式存储。  
> 
> 1）前端优化  
> ① 静态资源缓存：将活动页面上的所有可以静态的元素全部静态化，尽量减少动态元素；通过 CDN、浏览器缓存，来减少客户端向服务器端的数据请求。   
> ② 禁止重复提交：用户提交之后按钮置灰，禁止重复提交。  
> ③ 用户限流：在某一时间段内只允许用户提交一次请求，比如，采取 IP 限流。  
> 
> 2）中间层负载分发  
> 可利用负载均衡，比如 nginx 等工具，可以将并发请求分配到不同的服务器，从而提高了系统处理并发的能力。  
> 
> 3）控制层（网关层）  
> 限制同一个用户的访问频率，限制访问次数，防止多次恶意请求。  
> 
> 4）服务层  
> ① 业务服务器分离：比如，将秒杀业务系统和其他业务分离，单独放在高配服务器上，可以集中资源对访问请求抗压。   
> ② 采用 MQ（消息队列）缓存请求：MQ 具有削峰填谷的作用，可以把客户端的请求先导流到 MQ，程序在从 MQ 中进行消费（执行请求），这样可以避免短时间内大量请求，导致服务器程序无法响应的问题。   
> ③ 利用缓存应对读请求，比如，使用 Redis 等缓存，利用 Redis 可以分担数据库很大一部分压力。  
> 
> 5）数据库层  
> ① 合理使用数据库引擎  
> ② 合理设置事务隔离级别，合理使用事务  
> ③ 正确使用 SQL 语句和查询索引  
> ④ 合理分库分表  
> ⑤ 使用数据库中间件实现数据库读写分离  
> ⑥ 设置数据库主从读写分离  
> 
> 高并发基本思路  
> 1、从客户端看，尽量减少请求数量，比如依靠客户端自身的缓存或处理能力；尽量减少对服务端资源的不必要耗费，比如重复使用某些资源，如连接池客户端处理的基本原则就是 —— 能不访问服务端就不要访问。  
> 2、从服务端看，增加资源供给，比如更大的网络带宽，使用更高配置的服务器，使用高性能的 Web 服务器，使用高性能的数据库；请求分流，比如使用集群、分布式系统架构；应用优化，比如使用更高效的编程语言，优化处理业务逻辑的算法，优化访问数据库的 SQL；基本原则就是分而治之，并提高单个请求的处理速度。  
> 